<notebook version="1"><head/><ipython-log id="default-log"/><sheet id="chapter10">
<title>Linear Algebra</title>
<para>When SciPy is built using the optimized ATLAS LAPACK and BLAS libraries,
it has very fast linear algebra capabilities. If you dig deep enough, all of the
raw lapack and blas libraries are available for your use for even more speed. In
this section, some easier-to-use interfaces to these routines are described.
</para>
<para>
All of these linear algebra routines expect an object that can be converted into
a 2-dimensional array. The output of these routines is also a two-dimensional
array. There is a matrix class defined in Numeric that scipy inherits and
extends. You can initialize this class with an appropriate Numeric array in
order to get objects for which multiplication is matrix-multiplication instead
of the default, element-by-element multiplication. 
</para>

<section>
<title>Matrix Class</title>
<para>The matrix class is initialized with the SciPy command mat which is
just convenient short-hand for Matrix.Matrix. If you are going to be doing a lot
of matrix-math, it is convenient to convert arrays into matrices using this
command. One convencience of using the mat command is that you can enter
two-dimensional matrices using MATLAB-like syntax with commas or spaces
separating columns and semicolons separting rows as long as the matrix is placed
in a string passed to mat. 
</para>
</section>

<section>
<title>Basic routines</title>
<section>
<title>Finding Inverse</title>
<para>The inverse of a matrix \mathbf{A} is the matrix \mathbf{B} such that
\mathbf{AB}=\mathbf{I} where \mathbf{I} is the identity matrix consisting of
ones down the main diagonal. Usually \mathbf{B} is denoted
\mathbf{B}=\mathbf{A}^{-1}. In SciPy, the matrix inverse of the Numeric array,
A, is obtained using linalg.inv(A), or using A.I if A is a Matrix. For example,
let \mathbf{A=}\left[\begin{array}{ccc}
1 &amp; 3 &amp; 5\\
2 &amp; 5 &amp; 1\\
2 &amp; 3 &amp; 8\end{array}\right]then \mathbf{A^{-1}=\frac{1}{25}\left[\begin{array}{ccc}
-37 &amp; 9 &amp; 22\\
14 &amp; 2 &amp; -9\\
4 &amp; -3 &amp; 1\end{array}\right]=\left[\begin{array}{ccc}
-1.48 &amp; 0.36 &amp; 0.88\\
0.56 &amp; 0.08 &amp; -0.36\\
0.16 &amp; -0.12 &amp; 0.04\end{array}\right].} The following example demonstrates this computation in SciPy
</para>
<para> XXX: ipython
&gt;&gt;&gt; A = mat('[1 3 5; 2 5 1; 2 3 8]')
&gt;&gt;&gt; A
Matrix([[1, 3, 5],
       [2, 5, 1],
       [2, 3, 8]])
&gt;&gt;&gt; A.I
Matrix([[-1.48,  0.36,  0.88],
       [ 0.56,  0.08, -0.36],
       [ 0.16, -0.12,  0.04]])
&gt;&gt;&gt; linalg.inv(A)
array([[-1.48,  0.36,  0.88],
       [ 0.56,  0.08, -0.36],
       [ 0.16, -0.12,  0.04]])
</para>
</section>

<section>
<title>Solving linear system</title>
<para>Solving linear systems of equations is straightforward using the scipy
command linalg.solve. This command expects an input matrix and a right-hand-side
vector. The solution vector is then computed. An option for entering a symmetrix
matrix is offered which can speed up the processing when applicable. As an
example, suppose it is desired to solve the following simultaneous equations:
x+3y+5z We could find the solution vector using a matrix inverse:
\left[\begin{array}{c}
x\\
y\\
z\end{array}\right]=\left[\begin{array}{ccc}
1 &amp; 3 &amp; 5\\
2 &amp; 5 &amp; 1\\
2 &amp; 3 &amp; 8\end{array}\right]^{-1}\left[\begin{array}{c}
10\\
8\\
3\end{array}\right]=\frac{1}{25}\left[\begin{array}{c}
-232\\
129\\
19\end{array}\right]=\left[\begin{array}{c}
-9.28\\
5.16\\
0.76\end{array}\right]. However, it is better to use the linalg.solve command
which can be faster and more numerically stable. In this case it gives the same
answer as shown in the following example: 
</para>
<para>XXX: ipython
&gt;&gt;&gt; A = mat('[1 3 5; 2 5 1; 2 3 8]')
&gt;&gt;&gt; b = mat('[10;8;3]')
&gt;&gt;&gt; A.I*b
Matrix([[-9.28],
       [ 5.16],
       [ 0.76]])
&gt;&gt;&gt; linalg.solve(A,b)
array([[-9.28],
       [ 5.16],
       [ 0.76]])
</para>
</section>

<section>
<title>Finding Determinant</title>
<para>The determinant of a square matrix \mathbf{A} is often denoted
\left|\mathbf{A}\right| and is a quantity often used in linear algebra. Suppose
a_{ij} are the elements of the matrix \mathbf{A} and let
M_{ij}=\left|\mathbf{A}_{ij}\right| be the determinant of the matrix left by
removing the i^{\textrm{th}} row and j^{\textrm{th}}column from \mathbf{A}. Then
for any row i,\left|\mathbf{A}\right|=\sum_{j}\left(-1\right)^{i+j}a_{ij}M_{ij}.
This is a recursive way to define the determinant where the base case is defined
by accepting that the determinant of a 1\times1 matrix is the only matrix
element. In SciPy the determinant can be calculated with linalg.det. For
example, the determinant of \mathbf{A=}\left[\begin{array}{ccc}
1 &amp; 3 &amp; 5\\
2 &amp; 5 &amp; 1\\
2 &amp; 3 &amp; 8\end{array}\right]is \left|\mathbf{A}\right| In SciPy this is
computed as shown in this example:
</para>
<para> XXX: ipython
&gt;&gt;&gt; A = mat('[1 3 5; 2 5 1; 2 3 8]')
&gt;&gt;&gt; linalg.det(A)
-25.000000000000004
</para>
</section>

<section>
<title>Computing norms</title>
<para>Matrix and vector norms can also be computed with SciPy. A wide
range of norm definitions are available using different parameters to the order
argument of linalg.norm. This function takes a rank-1 (vectors) or a rank-2
(matrices) array and an optional order argument (default is 2). Based on these
inputs a vector or matrix norm of the requested order is computed. 
</para>
<para>
For vector x, the order parameter can be any real number including inf or -inf. The computed norm is \left\Vert \mathbf{x}\right\Vert =\left\{ \begin{array}{cc}
\max\left|x_{i}\right| &amp; \textrm{ord}=\textrm{inf}\\
\min\left|x_{i}\right| &amp; \textrm{ord}=-\textrm{inf}\\
\left(\sum_{i}\left|x_{i}\right|^{\textrm{ord}}\right)^{1/\textrm{ord}} &amp;
\left|\textrm{ord}\right|&lt;\infty.\end{array}\right.
</para>
<para>
For matrix \mathbf{A} the only valid values for norm are \pm2,\pm1, \pm inf, and
'fro' (or 'f') Thus, \left\Vert \mathbf{A}\right\Vert =\left\{ \begin{array}{cc}
\max_{i}\sum_{j}\left|a_{ij}\right| &amp; \textrm{ord}=\textrm{inf}\\
\min_{i}\sum_{j}\left|a_{ij}\right| &amp; \textrm{ord}=-\textrm{inf}\\
\max_{j}\sum_{i}\left|a_{ij}\right| &amp; \textrm{ord}=1\\
\min_{j}\sum_{i}\left|a_{ij}\right| &amp; \textrm{ord}=-1\\
\max\sigma_{i} &amp; \textrm{ord}=2\\
\min\sigma_{i} &amp; \textrm{ord}=-2\\
\sqrt{\textrm{trace}\left(\mathbf{A}^{H}\mathbf{A}\right)} &amp;
\textrm{ord}=\textrm{'fro'}\end{array}\right. where \sigma_{i} are the singular
values of \mathbf{A}. 
</para>
</section>

<section>
<title>Solving linear least-squares problems and pseudo-inverses</title>
<para>Linear least-squares problems occur in many branches of applied
mathematics. In this problem a set of linear scaling coefficients is sought that
allow a model to fit data. In particular it is assumed that data y_{i} is
related to data \mathbf{x}_{i} through a set of coefficients c_{j} and model
functions f_{j}\left(\mathbf{x}_{i}\right) via the model
y_{i}=\sum_{j}c_{j}f_{j}\left(\mathbf{x}_{i}\right)+\epsilon_{i} where
\epsilon_{i} represents uncertainty in the data. The strategy of least squares
is to pick the coefficients c_{j} to minimize
J\left(\mathbf{c}\right)=\sum_{i}\left|y_{i}-\sum_{j}c_{j}f_{j}\left(x_{i}\right)\right|^{2}. 
</para>
<para>Theoretically, a global minimum will occur when \frac{\partial J}{\partial
c_{n}^{*}}=0=\sum_{i}\left(y_{i}-\sum_{j}c_{j}f_{j}\left(x_{i}\right)\right)\left(-f_{n}^{*}\left(x_{i}\right)\right)or
\sum_{j}c_{j}\sum_{i}f_{j}\left(x_{i}\right)f_{n}^{*}\left(x_{i}\right) where
\left\{ \mathbf{A}\right\} _{ij}=f_{j}\left(x_{i}\right). When \mathbf{A^{H}A}
is invertible, then
\mathbf{c}=\left(\mathbf{A}^{H}\mathbf{A}\right)^{-1}\mathbf{A}^{H}\mathbf{y}=\mathbf{A}^{\dagger}\mathbf{y}
where \mathbf{A}^{\dagger} is called the pseudo-inverse of \mathbf{A}. Notice
that using this definition of \mathbf{A} the model can be written
\mathbf{y}=\mathbf{Ac}+\boldsymbol{\epsilon}.The command linalg.lstsq will solve
the linear least squares problem for \mathbf{c} given \mathbf{A} and \mathbf{y}.
In addition linalg.pinv or linalg.pinv2 (uses a different method based on
singular value decomposition) will find \mathbf{A}^{\dagger} given \mathbf{A}. 
</para>
<para>The following example and figure demonstrate the use of linalg.lstsq and
linalg.pinv for solving a data-fitting problem. The data shown below were
generated using the model: y_{i}=c_{1}e^{-x_{i}}+c_{2}x_{i} where x_{i}=0.1i for
i=1\ldots10, c_{1}=5, and c_{2}=4. Noise is added to y_{i} and the coefficients
c_{1} and c_{2} are estimated using linear least squares. 
</para>
<para> XXX: ipython
c1,c2= 5.0,2.0
i = r_[1:11]
xi = 0.1*i
yi = c1*exp(-xi)+c2*xi
zi = yi + 0.05*max(yi)*randn(len(yi))

A = c_[exp(-xi)[:,NewAxis],xi[:,NewAxis]]
c,resid,rank,sigma = linalg.lstsq(A,zi)

xi2 = r_[0.1:1.0:100j]
yi2 = c[0]*exp(-xi2) + c[1]*xi2

xplt.plot(xi,zi,'x',xi2,yi2)
xplt.limits(0,1.1,3.0,5.5)
xplt.xlabel('x_i')
xplt.title('Data fitting with linalg.lstsq')
xplt.eps('lstsq_fit')
</para>
<para>
[Graphics file: lstsq_fit.eps]
</para>
</section>

<section>
<title>Generalized inverse</title>
<para>The generalized inverse is calculated using the command linalg.pinv
or linalg.pinv2. These two commands differ in how they compute the generalized
inverse. The first uses the linalg.lstsq algorithm while the second uses
singular value decomposition. Let \mathbf{A} be an M\times N matrix, then if M&gt;N
the generalized inverse is
\mathbf{A}^{\dagger}=\left(\mathbf{A}^{H}\mathbf{A}\right)^{-1}\mathbf{A}^{H}
while if M&lt;N matrix the generalized inverse is
\mathbf{A}^{\#}=\mathbf{A}^{H}\left(\mathbf{A}\mathbf{A}^{H}\right)^{-1}. In
both cases for M=N, then \mathbf{A}^{\dagger}=\mathbf{A}^{\#}=\mathbf{A}^{-1}as
long as \mathbf{A} is invertible. 
</para>
</section>
</section>

<section>
<title>Decompositions</title>
<para>In many applications it is useful to decompose a matrix using other
representations. There are several decompositions supported by SciPy. 
</para>

<section>
<title>Eigenvalues and eigenvectors</title>
<para>The eigenvalue-eigenvector problem is one of the most commonly
employed linear algebra operations. In one popular form, the
eigenvalue-eigenvector problem is to find for some square matrix \mathbf{A}
scalars \lambda  and corresponding vectors \mathbf{v} such that
\mathbf{Av}=\lambda\mathbf{v}. For an N\times N matrix, there are N (not
necessarily distinct) eigenvalues --- roots of the (characteristic) polynomial
\left|\mathbf{A}-\lambda\mathbf{I}\right|=0.
</para>
<para>The eigenvectors, \mathbf{v}, are also sometimes called right eigenvectors to
distinguish them from another set of left eigenvectors that satisfy
\mathbf{v}_{L}^{H}\mathbf{A}=\lambda\mathbf{v}_{L}^{H}or
\mathbf{A}^{H}\mathbf{v}_{L}=\lambda^{*}\mathbf{v}_{L}. With it's default
optional arguments, the command linalg.eig returns \lambda  and \mathbf{v}.
However, it can also return \mathbf{v}_{L} and just \lambda  by itself
(linalg.eigvals returns just \lambda  as well). 
</para>
<para>In addtion, linalg.eig can also solve the more general eigenvalue problem
\mathbf{Av}for square matrices \mathbf{A} and \mathbf{B}. The standard
eigenvalue problem is an example of the general eigenvalue problem for
\mathbf{B}=\mathbf{I}. When a generalized eigenvalue problem can be solved, then
it provides a decomposition of \mathbf{A} as
\mathbf{A}=\mathbf{BV}\boldsymbol{\Lambda}\mathbf{V}^{-1} where \mathbf{V} is
the collection of eigenvectors into columns and \boldsymbol{\Lambda} is a
diagonal matrix of eigenvalues. 
</para>
<para>By definition, eigenvectors are only defined up to a constant scale factor. In
SciPy, the scaling factor for the eigenvectors is chosen so that \left\Vert
\mathbf{v}\right\Vert ^{2}=\sum_{i}v_{i}^{2}=1. 
</para>
<para>As an example, consider finding the eigenvalues and eigenvectors of the matrix
\mathbf{A}=\left[\begin{array}{ccc}
1 &amp; 5 &amp; 2\\
2 &amp; 4 &amp; 1\\
3 &amp; 6 &amp; 2\end{array}\right]. The characteristic polynomial is
\left|\mathbf{A}-\lambda\mathbf{I}\right| The roots of this polynomial are the
eigenvalues of \mathbf{A}: \lambda_{1} The eigenvectors corresponding to each
eigenvalue can be found using the original equation. The eigenvectors associated
with these eigenvalues can then be found. 
</para>
<para>XXX: ipython
&gt;&gt;&gt; A = mat('[1 5 2; 2 4 1; 3 6 2]')
&gt;&gt;&gt; la,v = linalg.eig(A)
&gt;&gt;&gt; l1,l2,l3 = la
&gt;&gt;&gt; print l1, l2, l3
(7.95791620491+0j) (-1.25766470568+0j) (0.299748500767+0j)

&gt;&gt;&gt; print v[:,0]
array([-0.5297, -0.4494, -0.7193])
&gt;&gt;&gt; print v[:,1]
[-0.9073  0.2866  0.3076]
&gt;&gt;&gt; print v[:,2]
[ 0.2838 -0.3901  0.8759]
&gt;&gt;&gt; print sum(abs(v**2),axis=0)
[ 1.  1.  1.]

&gt;&gt;&gt; v1 = mat(v[:,0]).T
&gt;&gt;&gt; print max(ravel(abs(A*v1-l1*v1)))
4.4408920985e-16
</para>
</section>

<section>
<title>Singular value decomposition</title>
<para>Singular Value Decompostion (SVD) can be thought of as an extension of the
eigenvalue problem to matrices that are not square. Let \mathbf{A} be an M\times
N matrix with M and N arbitrary. The matrices \mathbf{A}^{H}\mathbf{A} and
\mathbf{A}\mathbf{A}^{H} are square hermitian matrices<footnote>A hermition matrix
\mathbf{D} satisfies \mathbf{D}^{H}=\mathbf{D}.</footnote> of size N\times N and M\times M
respectively. It is known that the eigenvalues of square hermitian matrices are
real and non-negative. In addtion, there are at most \min\left(M,N\right)
identical non-zero eigenvalues of \mathbf{A}^{H}\mathbf{A} and
\mathbf{A}\mathbf{A}^{H}. Define these positive eigenvalues as \sigma_{i}^{2}.
The square-root of these are called singular values of \mathbf{A}. The
eigenvectors of \mathbf{A}^{H}\mathbf{A} are collected by columns into an
N\times N unitary<footnote>A unitary matrix \mathbf{D} satisfies
\mathbf{D}^{H}\mathbf{D}=\mathbf{I}=\mathbf{D}\mathbf{D}^{H} so that
\mathbf{D}^{-1}=\mathbf{D}^{H}.</footnote> matrix \mathbf{V} while the eigenvectors of
\mathbf{A}\mathbf{A}^{H} are collected by columns in the unitary matrix
\mathbf{U}, the singular values are collected in an M\times N zero matrix
\mathbf{\boldsymbol{\Sigma}} with main diagonal entries set to the singular
values. Then \mathbf{A=U}\boldsymbol{\Sigma}\mathbf{V}^{H} is the singular-value
decomposition of \mathbf{A}. Every matrix has a singular value decomposition.
Sometimes, the singular values are called the spectrum of \mathbf{A}. The
command linalg.svd will return \mathbf{U}, \mathbf{V}^{H}, and \sigma_{i} as an
array of the singular values. To obtain the matrix \mathbf{\Sigma} use
linalg.diagsvd. The following example illustrates the use of linalg.svd. 
</para>
<para>XXX: ipython
&gt;&gt;&gt; A = mat('[1 3 2; 1 2 3]')
&gt;&gt;&gt; M,N = A.shape
&gt;&gt;&gt; U,s,Vh = linalg.svd(A)
&gt;&gt;&gt; Sig = mat(diagsvd(s,M,N))
&gt;&gt;&gt; U, Vh = mat(U), mat(Vh)
&gt;&gt;&gt; print U
Matrix([[-0.7071, -0.7071],
       [-0.7071,  0.7071]])
&gt;&gt;&gt; print Sig
Matrix([[ 5.1962,  0.    ,  0.    ],
       [ 0.    ,  1.    ,  0.    ]])
&gt;&gt;&gt; print Vh
Matrix([[-0.2722, -0.6804, -0.6804],
       [-0.    , -0.7071,  0.7071],
       [-0.9623,  0.1925,  0.1925]])

&gt;&gt;&gt; print A
Matrix([[1, 3, 2],
       [1, 2, 3]])
&gt;&gt;&gt; print U*Sig*Vh
Matrix([[ 1.,  3.,  2.],
       [ 1.,  2.,  3.]])
</para>
</section>

<section>
<title>LU decomposition</title>
<para>The LU decompostion finds a representation for the M\times N matrix
\mathbf{A} as \mathbf{A}=\mathbf{PLU} where \mathbf{P} is an M\times M
permutation matrix (a permutation of the rows of the identity matrix),
\mathbf{L} is in M\times K lower triangular or trapezoidal matrix
(K=\min\left(M,N\right)) with unit-diagonal, and \mathbf{U} is an upper
triangular or trapezoidal matrix. The SciPy command for this decomposition is
linalg.lu. 
</para>
<para>
Such a decomposition is often useful for solving many simultaneous equations
where the left-hand-side does not change but the right hand side does. For
example, suppose we are going to solve \mathbf{A}\mathbf{x}_{i}=\mathbf{b}_{i}
for many different \mathbf{b}_{i}. The LU decomposition allows this to be
written as \mathbf{PLUx}_{i}=\mathbf{b}_{i}. Because \mathbf{L} is
lower-triangular, the equation can be solved for \mathbf{U}\mathbf{x}_{i} and
finally \mathbf{x}_{i} very rapidly using forward- and back-substitution. An
initial time spent factoring \mathbf{A} allows for very rapid solution of
similar systems of equations in the future. If the intent for performing LU
decomposition is for solving linear systems then the command linalg.lu_factor
should be used followed by repeated applications of the command linalg.lu_solve
to solve the system for each new right-hand-side. 
</para>
</section>

<section>
<title>Cholesky decomposition</title>
<para>Cholesky decomposition is a special case of LU decomposition
applicable to Hermitian positive definite matrices. When
\mathbf{A}=\mathbf{A}^{H} and \mathbf{x}^{H}\mathbf{Ax}\geq0 for all \mathbf{x},
then decompositions of \mathbf{A} can be found so that \mathbf{A}where
\mathbf{L} is lower-triangular and \mathbf{U} is upper triangular. Notice that
\mathbf{L}=\mathbf{U}^{H}. The command linagl.cholesky computes the cholesky
factorization. For using cholesky factorization to solve systems of equations
there are also linalg.cho_factor and linalg.cho_solve routines that work
similarly to their LU decomposition counterparts. 
</para>
</section>

<section>
<title>QR decomposition</title>
<para>The QR decomposition (sometimes called a polar decomposition) works for
any M\times N array and finds an M\times M unitary matrix \mathbf{Q} and an
M\times N upper-trapezoidal matrix \mathbf{R} such that \mathbf{A=QR}. Notice
that if the SVD of \mathbf{A} is known then the QR decomposition can be found
\mathbf{A}=\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}^{H}=\mathbf{QR} implies that
\mathbf{Q}=\mathbf{U} and \mathbf{R}=\boldsymbol{\Sigma}\mathbf{V}^{H}. Note,
however, that in SciPy independent algorithms are used to find QR and SVD
decompositions. The command for QR decomposition is linalg.qr. 
</para>
</section>

<section>
<title>Schur decomposition</title>
<para>For a square N\times N matrix, \mathbf{A}, the Schur decomposition finds
(not-necessarily unique) matrices \mathbf{T} and \mathbf{Z} such that
\mathbf{A}=\mathbf{ZT}\mathbf{Z}^{H} where \mathbf{Z} is a unitary matrix and
\mathbf{T} is either upper-triangular or quasi-upper triangular depending on
whether or not a real schur form or complex schur form is requested. For a real
schur form both \mathbf{T} and \mathbf{Z} are real-valued when \mathbf{A} is
real-valued. When \mathbf{A} is a real-valued matrix the real schur form is only
quasi-upper triangular because 2\times2 blocks extrude from the main diagonal
corresponding to any complex-valued eigenvalues. The command linalg.schur finds
the Schur decomposition while the command linalg.rsf2csf converts \mathbf{T} and
\mathbf{Z} from a real Schur form to a complex Schur form. The Schur form is
especially useful in calculating functions of matrices. 
</para>
<para>The following example illustrates the schur decomposition:
</para>
<para>XXX: ipython
&gt;&gt;&gt; A = mat('[1 3 2; 1 4 5; 2 3 6]')
&gt;&gt;&gt; T,Z = linalg.schur(A)
&gt;&gt;&gt; T1,Z1 = linalg.schur(A,'complex')
&gt;&gt;&gt; T2,Z2 = linalg.rsf2csf(T,Z)
&gt;&gt;&gt; print T
Matrix([[ 9.9001,  1.7895, -0.655 ],
       [ 0.    ,  0.5499, -1.5775],
       [ 0.    ,  0.5126,  0.5499]])
&gt;&gt;&gt; print T2
Matrix([[ 9.9001+0.j    , -0.3244+1.5546j, -0.8862+0.569j ],
       [ 0.    +0.j    ,  0.5499+0.8993j,  1.0649-0.j    ],
       [ 0.    +0.j    ,  0.    +0.j    ,  0.5499-0.8993j]])
&gt;&gt;&gt; print abs(T1-T2) # different
[[ 0.      2.1184  0.1949]
 [ 0.      0.      1.2676]
 [ 0.      0.      0.    ]]
&gt;&gt;&gt; print abs(Z1-Z2) # different
[[ 0.0683  1.1175  0.1973]
 [ 0.1186  0.5644  0.247 ]
 [ 0.1262  0.7645  0.1916]]
&gt;&gt;&gt; T,Z,T1,Z1,T2,Z2 = map(mat,(T,Z,T1,Z1,T2,Z2))
&gt;&gt;&gt; print abs(A-Z*T*Z.H)
Matrix([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.]])
&gt;&gt;&gt; print abs(A-Z1*T1*Z1.H)
Matrix([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.]])
&gt;&gt;&gt; print abs(A-Z2*T2*Z2.H)
Matrix([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.]])
</para>
</section>
</section>

<section>
<title>Matrix Functions</title>
<para>Consider the function f\left(x\right) with Taylor series expansion
f\left(x\right)=\sum_{k=0}^{\infty}\frac{f^{\left(k\right)}\left(0\right)}{k!}x^{k}.
A matrix function can be defined using this Taylor series for the square matrix
\mathbf{A} as
f\left(\mathbf{A}\right)=\sum_{k=0}^{\infty}\frac{f^{\left(k\right)}\left(0\right)}{k!}\mathbf{A}^{k}.
While, this serves as a useful representation of a matrix function, it is rarely
the best way to calculate a matrix function. 
</para>

<section>
<title>Exponential and logarithm functions</title>
<para>The matrix exponential is one of the more common matrix functions. It can
be defined for square matrices as
e^{\mathbf{A}}=\sum_{k=0}^{\infty}\frac{1}{k!}\mathbf{A}^{k}. The command
linalg.expm3 uses this Taylor series definition to compute the matrix
exponential. Due to poor convergence properties it is not often used. 
</para>
<para>Another method to compute the matrix exponential is to find an eigenvalue
decomposition of \mathbf{A}:
\mathbf{A}=\mathbf{V}\boldsymbol{\Lambda}\mathbf{V}^{-1} and note that
e^{\mathbf{A}}=\mathbf{V}e^{\boldsymbol{\Lambda}}\mathbf{V}^{-1} where the
matrix exponential of the diagonal matrix \boldsymbol{\Lambda} is just the
exponential of its elements. This method is implemented in linalg.expm2. 
</para>
<para>The preferred method for implementing the matrix exponential is to use scaling
and a Pad√© approximation for e^{x}. This algorithm is implemented as
linalg.expm. 
</para>
<para>The inverse of the matrix exponential is the matrix logarithm defined as the
inverse of the matrix exponential.
\mathbf{A}\equiv\exp\left(\log\left(\mathbf{A}\right)\right). The matrix
logarithm can be obtained with linalg.logm. 
</para>
</section>

<section>
<title>Trigonometric functions</title>
<para>The trigonometric functions \sin , \cos , and \tan  are implemented
for matrices in linalg.sinm, linalg.cosm, and linalg.tanm respectively. The
matrix sin and cosine can be defined using Euler's identity as
\sin\left(\mathbf{A}\right) The tangent is
\tan\left(x\right)=\frac{\sin\left(x\right)}{\cos\left(x\right)}=\left[\cos\left(x\right)\right]^{-1}\sin\left(x\right)and
so the matrix tangent is defined as
\left[\cos\left(\mathbf{A}\right)\right]^{-1}\sin\left(\mathbf{A}\right).
</para>
</section>

<section>
<title>Hyperbolic trigonometric functions</title>
<para>The hyperbolic trigonometric functions \sinh , \cosh , and \tanh
can also be defined for matrices using the familiar definitions:
\sinh\left(\mathbf{A}\right) These matrix functions can be found using
linalg.sinhm, linalg.coshm, and linalg.tanhm. 
</para>
</section>

<section>
<title>Arbitrary function</title>
<para>Finally, any arbitrary function that takes one complex number and returns
a complex number can be called as a matrix function using the command
linalg.funm. This command takes the matrix and an arbitrary Python function. It
then implements an algorithm from Golub and Van Loan's book "Matrix
Computations" to compute function applied to the matrix using a Schur
decomposition. Note that the function needs to accept complex numbers as input
in order to work with this algorithm. For example the following code computes
the zeroth-order Bessel function applied to a matrix.
</para>
<para>XXX: ipython
&gt;&gt;&gt; A = rand(3,3)
&gt;&gt;&gt; B = linalg.funm(A,lambda x: special.jv(0,real(x)))
&gt;&gt;&gt; print A
[[ 0.0593  0.5612  0.4403]
 [ 0.8797  0.2556  0.1452]
 [ 0.964   0.9666  0.1243]]
&gt;&gt;&gt; print B
[[ 0.8206 -0.1212 -0.0612]
 [-0.1323  0.8256 -0.0627]
 [-0.2073 -0.1946  0.8516]]
</para>
</section>
</section>

</sheet></notebook>