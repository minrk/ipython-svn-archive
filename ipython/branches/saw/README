===============
IPython1 README
===============

:Author:  Brian E. Granger
:Contact: ellisonbg@gmail.com
:Date: April 24, 2007

.. contents::

Overview
========

This file gives an overview of the IPython1 kernel.  The IPython kernel exposes
Python over the network and allows parallel and distributed applications to be
developed, debugged, tested, executed and monitored interactively and
collaboratively.

For installation instructions, see the INSTALL file in this directory.

The IPython kernel is a component of a future release of IPython called
IPython1.  While IPython1 is not completed, the kernel module is fully
functional and can be used with current IPython versions.

The IPython kernel is essentially a network enabled Python instance and
consists of three components: the IPython Engine, the IPython Controller and
various Controller Clients.

IPython Engine
--------------

The IPython Engine is a Python instance that takes Python commands over a
network connection.  Eventually, the IPython Engine will be an IPython
instance, but for now, it is plain Python.  The Engine can also handle incoming
and outgoing Python objects send over a network connection.  When multiple
Engines are started, parallel and distributed computing becomes possible.  An
important feature of the IPython Engines is that they block while user code is
being executed.  Read on for how the Controller deals with this limitation to 
expose a nice clean asynchronous API to the user.

IPython Controller
------------------

The IPython Controller provides an interface for working with a set of Engines.
The basic notion of a controller is something to which engines can connect.
It manages a queue for each Engine and exposes a fully asynchronous interface
to each Engine.  Because the Controller listens on a network port for Engines
to connect to it, it must be started before any Engines are started.  The
Controller also provides a single point of contact for users who wish to
utilize the Engines connected to the Controller.  There are different ways 
(read interfaces) of working with a controller.  Currently we have two
default interfaces to the controller:  RemoteController and TaskController.

Controller Clients
------------------

For each controller interface, there is a corresponding client.  These clients
allow users to interact with a set of engines through the interface that
the controller exposes.

Getting Started
===============

To use IPython1 for parallel computing, you need to start one instance of the
controller and one or more instances of the engine.  The controller and each
engine can run on different machines or on the same machine.  Because of this,
there are many different possibilities for setting up the ip addresses and
ports used by the various processes.

Starting the controller and engine on your local machine
--------------------------------------------------------

This is the simplest configuration that can be used and is useful for testing
the system and on machines that have multiple cores and/or multple CPUs.  The
easiest way of doing this is using the ``ipcluster`` command::

	ipcluster -n 4
	
This will start an IPython controller and then 4 engines that connect to the 
controller.  Lastly, the script will print out the Python commands that you
can use to connect to the controller.  It is that easy.

Underneath the hood, the ``ipcluster`` script uses two other top-level scripts
that you can also use yourself.  These scripts are ``ipcontroller``, which
starts the controller and ``ipengine`` which starts an engine.  To use these
scripts to start things on your local machine, do the following.

First start the controller::

	ipcontroller &
	
Next, start however many instances of the engine you want to use::

	ipengine &

*WARNING*: the order of the above operations is very important.  You *must*
 start the controller before the engines, since the engines connect to the
 controller as they get started.

On some platforms you may need to give these commands in the form
``(ipcontroller &)`` and ``(ipengine &)`` for them to work properly.  The
engines should start and automatically connect to the controller on the default
ports, which are chosen for this type of setup.  The logs are fairly verbose
and you should see various messages about the engines being registered with the
controller.  You are now ready to use the controller and engines from IPython.

Starting the controller and engines on different machines
---------------------------------------------------------

To start the engines and controller on different machines you simply need to
specify the ip addresses where the controller is running.  Let's say you want
to start the controller on host0 and the engines on hosts1 - host16.  First
start the controller on host0::

	host0> ipcontroller &
	
Next, start the engines on host1 - host16 specifying the where the controller
is running::

	host1> ipengine --controller-ip=host0 &
	
Repeat this on host2 - host16 and you should be ready to go.

Specifying custom ports
-----------------------

The controller can be configured to use different ports that the default.
Both the controller and engines must be told to use the same port::

	ipcontroller --engine-port=10001              # listen on 10001
	ipengine --controller-port=10001              # connect to 10001
	
Starting engines using mpirun
-----------------------------

The IPython engine can be started using mpirun/mpiexec, even if the engines
don't call MPI_Init() or use the MPI API.  This is supported on modern MPI
implementations like OpenMPI.  On a system with MPI installed you can do::

	mpirun -n 4 ipengine --controller-port=10000 --controller-ip=host0
	

Using the Controller and Engines from a Client
==============================================

The best source of documentation about how to use the controller and engines
is the docstrings of kernel modules.  Eventually, we will provide more 
details in this readme.

The RemoteController client
---------------------------

Once a controller and engines are running you can connect to it using the 
`RemoteController` class.  This class provides a client for the MultiEngine
interface.  Don't worry about what this means for now, it will become
clear as you use it.

First import the ipython1 client and create a RemoteController instance::

	>>> import ipython1.kernel.api as kernel
	>>> rc = kernel.RemoteController(('127.0.0.1',10105))
	
Now you can see what engines are registered with the controller::

	>>> rc.getIDs()
	[0,1,2,3]

If this works, you are ready to go.  Start next by looking at the docstrings
for the following commands:

1. execute
2. push/pull
3. scatter/gather

The TaskController client
-------------------------

The TaskController interface to the controller presents the engines as a fault
tolerant, dynamic load-balancing system.  You can even have a
RemoteController and a TaskController simultaneously connected to the same
controller.  But, be warned, the tasks will share the namespace on the 
engines with the RemoteController.  But, this is actually a feature that allows
you to do some really nice things.

First create a TaskController objects::

	>>> import ipython1.kernel.api as kernel
	>>> tc = kernel.TaskController(('127.0.0.1',10113))
	
Note that the port is different than the RemoteController.  Now you can 
start submitting tasks by passing instances of kernel.Task to tc.run.

It is best to check out the examples of using the task farming stuff in 
ipython1/doc/examples.


Things To Do
============

Current:

* Planning with Min and Fernando.
* Begin to add tests for task controller and client.
* Convert standalone test scripts to using Fernando's new spawning stuff.
* Add capability to spawn engines using mpirun.
* Refactor the exceptions in xmlrpcutil.

Ongoing:

* is it a message, result or command in the new core?
* should the id attribute of engines be called engineID? (there are lots of ids around)
* Write tutorial for task farming stuff.
* Code review for taskpb.py.
* Have Min update multienginehttp/taskhttp so tests pass (barrier stuff)
* clean up mpi.so